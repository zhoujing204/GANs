{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "876e0348-c280-46bf-8802-ccabb04dcfcc",
   "metadata": {},
   "source": [
    "# Lab 2, Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700e687c",
   "metadata": {
    "height": 249,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "from diffusion_utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d229a",
   "metadata": {},
   "source": [
    "# Setting Things Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23507e17",
   "metadata": {
    "height": 1354,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):  # cfeat - context features\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        # number of input channels, number of intermediate feature maps and number of classes\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height  #assume h == w. must be divisible by 4, so 28,24,20,16...\n",
    "\n",
    "        # Initialize the initial convolutional layer\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        # Initialize the down-sampling path of the U-Net with two levels\n",
    "        self.down1 = UnetDown(n_feat, n_feat)        # down1 #[10, 256, 8, 8]\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)    # down2 #[10, 256, 4,  4]\n",
    "\n",
    "         # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\n",
    "\n",
    "        # Embed the timestep and context labels with a one-layer fully connected neural network\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)\n",
    "\n",
    "        # Initialize the up-sampling path of the U-Net with three levels\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.h//4, self.h//4), # up-sample\n",
    "            nn.GroupNorm(8, 2 * n_feat), # normalize\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # Initialize the final convolutional layers to map to the same number of channels as the input image\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            nn.GroupNorm(8, n_feat), # normalize\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # map to same number of channels as input\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        x : (batch, n_feat, h, w) : input image\n",
    "        t : (batch, n_cfeat)      : time step\n",
    "        c : (batch, n_classes)    : context label\n",
    "        \"\"\"\n",
    "        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on\n",
    "\n",
    "        # pass the input image through the initial convolutional layer\n",
    "        x = self.init_conv(x)\n",
    "        # pass the result through the down-sampling path\n",
    "        down1 = self.down1(x)       #[10, 256, 8, 8]\n",
    "        down2 = self.down2(down1)   #[10, 256, 4, 4]\n",
    "\n",
    "        # convert the feature maps to a vector and apply an activation\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "\n",
    "        # mask out context if context_mask == 1\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "\n",
    "        # embed context and timestep\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        #print(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
    "\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54c3a942",
   "metadata": {
    "height": 334,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "# diffusion hyperparameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = 5 # context vector is of size 5\n",
    "height = 16 # 16x16 image\n",
    "save_dir = './weights/'\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 100\n",
    "n_epoch = 32\n",
    "lrate=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a705d0a8",
   "metadata": {
    "height": 113,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct DDPM noise schedule\n",
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()\n",
    "ab_t[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bc9001e",
   "metadata": {
    "height": 62,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct model\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34ea64",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25073394",
   "metadata": {
    "height": 96,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sprite shape: (89400, 16, 16, 3)\n",
      "labels shape: (89400, 5)\n"
     ]
    }
   ],
   "source": [
    "# load dataset and construct optimizer\n",
    "dataset = CustomDataset(\"./sprites_1788_16x16.npy\", \"./sprite_labels_nc_1788_16x16.npy\", transform, null_context=False)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ac0a655",
   "metadata": {
    "height": 79,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function: perturbs an image to a specified noise level\n",
    "def perturb_input(x, t, noise):\n",
    "    return ab_t.sqrt()[t, None, None, None] * x + (1 - ab_t[t, None, None, None]).sqrt() * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bc9f45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Noise Schedule Analysis ===\n",
      "Beta range: [0.000100, 0.020000]\n",
      "Timesteps: 1000\n",
      "\n",
      "   t |      β_t |      α_t |      ᾱ_t |     √ᾱ_t |   √(1-ᾱ_t)\n",
      "-----------------------------------------------------------------\n",
      "   0 | 0.000100 | 0.999900 | 1.000000 | 1.000000 |   0.000000\n",
      "   1 | 0.000120 | 0.999880 | 0.999780 | 0.999890 |   0.014830\n",
      "  50 | 0.001095 | 0.998905 | 0.969976 | 0.984874 |   0.173274\n",
      " 100 | 0.002090 | 0.997910 | 0.895232 | 0.946167 |   0.323679\n",
      " 250 | 0.005075 | 0.994925 | 0.521750 | 0.722323 |   0.691556\n",
      " 500 | 0.010050 | 0.989950 | 0.077992 | 0.279271 |   0.960212\n",
      " 750 | 0.015025 | 0.984975 | 0.003319 | 0.057610 |   0.998339\n",
      " 999 | 0.019980 | 0.980020 | 0.000041 | 0.006385 |   0.999980\n",
      "1000 | 0.020000 | 0.980000 | 0.000040 | 0.006321 |   0.999980\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_noise_schedule_and_distribution(beta1=1e-4, beta2=2e-2, timesteps=1000):\n",
    "    \"\"\"Compute the exact noise schedule and x_t distribution\"\"\"\n",
    "\n",
    "    # 1. Beta schedule\n",
    "    b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1) + beta1\n",
    "\n",
    "    # 2. Alpha values\n",
    "    a_t = 1 - b_t\n",
    "\n",
    "    # 3. Cumulative alpha (the way it's computed in the code)\n",
    "    ab_t = torch.cumsum(a_t.log(), dim=0).exp()\n",
    "    ab_t[0] = 1  # Override first value\n",
    "\n",
    "    # 4. Alternative computation (should be equivalent)\n",
    "    ab_t_alternative = torch.cumprod(a_t, dim=0)\n",
    "\n",
    "    print(\"=== Noise Schedule Analysis ===\")\n",
    "    print(f\"Beta range: [{beta1:.6f}, {beta2:.6f}]\")\n",
    "    print(f\"Timesteps: {timesteps}\")\n",
    "\n",
    "    # Show key values\n",
    "    key_timesteps = [0, 1, 50, 100, 250, 500, 750, 999, 1000]\n",
    "    print(f\"\\n{'t':>4} | {'β_t':>8} | {'α_t':>8} | {'ᾱ_t':>8} | {'√ᾱ_t':>8} | {'√(1-ᾱ_t)':>10}\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    for t in key_timesteps:\n",
    "        if t < len(b_t):\n",
    "            beta_val = b_t[t].item()\n",
    "            alpha_val = a_t[t].item()\n",
    "            alpha_bar_val = ab_t[t].item()\n",
    "            sqrt_alpha_bar = alpha_bar_val**0.5\n",
    "            sqrt_one_minus_alpha_bar = (1 - alpha_bar_val)**0.5\n",
    "\n",
    "            print(f\"{t:4d} | {beta_val:8.6f} | {alpha_val:8.6f} | {alpha_bar_val:8.6f} | {sqrt_alpha_bar:8.6f} | {sqrt_one_minus_alpha_bar:10.6f}\")\n",
    "\n",
    "    return b_t, a_t, ab_t\n",
    "\n",
    "# Compute the schedule\n",
    "b_t, a_t, ab_t = compute_noise_schedule_and_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8388463e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== X_t Distribution Analysis ===\n",
      "Assuming image x₀ ~ N(0.5, 0.3²) for illustration\n",
      "   t |   E[x_t] |   Var[x_t] |   Std[x_t] | Signal Strength\n",
      "----------------------------------------------------------------------\n",
      "   0 |   0.5000 |   0.090000 |   0.300000 |          inf\n",
      " 100 |   0.4731 |   0.185339 |   0.430510 |       2.9232\n",
      " 500 |   0.1396 |   0.929027 |   0.963860 |       0.2908\n",
      "1000 |   0.0032 |   0.999964 |   0.999982 |       0.0063\n"
     ]
    }
   ],
   "source": [
    "def analyze_xt_distribution(x0_mean=0.5, x0_std=0.3, timesteps_to_analyze=[0, 100, 500, 1000]):\n",
    "    \"\"\"Analyze the distribution of x_t for different timesteps\"\"\"\n",
    "\n",
    "    print(\"\\n=== X_t Distribution Analysis ===\")\n",
    "    print(\"Assuming image x₀ ~ N(0.5, 0.3²) for illustration\")\n",
    "    print(f\"{'t':>4} | {'E[x_t]':>8} | {'Var[x_t]':>10} | {'Std[x_t]':>10} | Signal Strength\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for t in timesteps_to_analyze:\n",
    "        if t < len(ab_t):\n",
    "            # Coefficients from the perturb_input function\n",
    "            signal_coeff = ab_t[t].sqrt().item()      # √ᾱ_t\n",
    "            noise_coeff = (1 - ab_t[t]).sqrt().item() # √(1-ᾱ_t)\n",
    "\n",
    "            # x_t = signal_coeff * x₀ + noise_coeff * ε\n",
    "            # where x₀ ~ N(x0_mean, x0_std²) and ε ~ N(0, 1)\n",
    "\n",
    "            # Mean: E[x_t] = signal_coeff * E[x₀] + noise_coeff * E[ε]\n",
    "            mean_xt = signal_coeff * x0_mean + noise_coeff * 0\n",
    "\n",
    "            # Variance: Var[x_t] = signal_coeff² * Var[x₀] + noise_coeff² * Var[ε]\n",
    "            var_xt = (signal_coeff**2) * (x0_std**2) + (noise_coeff**2) * 1\n",
    "            std_xt = var_xt**0.5\n",
    "\n",
    "            # Signal-to-noise ratio\n",
    "            signal_strength = signal_coeff / noise_coeff if noise_coeff > 0 else float('inf')\n",
    "\n",
    "            print(f\"{t:4d} | {mean_xt:8.4f} | {var_xt:10.6f} | {std_xt:10.6f} | {signal_strength:12.4f}\")\n",
    "\n",
    "analyze_xt_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2004c5d7-4c6a-456d-844f-af8e98cd6b8e",
   "metadata": {},
   "source": [
    "#### This code will take hours to run on a CPU. We recommend you skip this step here and check the intermediate results below.\n",
    "If you decide to try it, you could download to your own machine. Be sure to change the cell type. \n",
    "Note, the CPU run time in the course is limited so you will not be able to fully train the network using the class platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5cee383-a28e-406b-8ab8-1a3e77e8a3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # training without context code\n",
    "\n",
    "# # set into train mode\n",
    "# nn_model.train()\n",
    "\n",
    "# for ep in range(n_epoch):\n",
    "#     print(f'epoch {ep}')\n",
    "\n",
    "#     # linearly decay learning rate\n",
    "#     optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
    "\n",
    "#     pbar = tqdm(dataloader, mininterval=2 )\n",
    "#     for x, _ in pbar:   # x: images\n",
    "#         optim.zero_grad()\n",
    "#         x = x.to(device)\n",
    "\n",
    "#         # perturb data\n",
    "#         noise = torch.randn_like(x)\n",
    "#         t = torch.randint(1, timesteps + 1, (x.shape[0],)).to(device)\n",
    "#         x_pert = perturb_input(x, t, noise)\n",
    "\n",
    "#         # use network to recover noise\n",
    "#         pred_noise = nn_model(x_pert, t / timesteps)\n",
    "\n",
    "#         # loss is mean squared error between the predicted and true noise\n",
    "#         loss = F.mse_loss(pred_noise, noise)\n",
    "#         loss.backward()\n",
    "\n",
    "#         optim.step()\n",
    "\n",
    "#     # save model periodically\n",
    "#     if ep%4==0 or ep == int(n_epoch-1):\n",
    "#         if not os.path.exists(save_dir):\n",
    "#             os.mkdir(save_dir)\n",
    "#         torch.save(nn_model.state_dict(), save_dir + f\"model_{ep}.pth\")\n",
    "#         print('saved model at ' + save_dir + f\"model_{ep}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265f9c6",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fa7aa8a",
   "metadata": {
    "height": 147,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function; removes the predicted noise (but adds some noise back in to avoid collapse)\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aadaf15-ca49-484a-b00b-d24f072d3d00",
   "metadata": {
    "height": 436,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample using standard algorithm\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(n_sample, save_rate=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)\n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = []\n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # sample some random noise to inject back in. For i = 1, don't add back in noise\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate ==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa42062-d7f4-49d9-954b-713165f81d19",
   "metadata": {},
   "source": [
    "#### View Epoch 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0c30c67",
   "metadata": {
    "height": 96,
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './weights//model_0.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# load in model weights and set to eval mode\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m nn_model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msave_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/model_0.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      3\u001b[39m nn_model.eval()\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoaded in Model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/serialization.py:1479\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1477\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1480\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1481\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1482\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1483\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1484\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './weights//model_0.pth'"
     ]
    }
   ],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_0.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8467c555",
   "metadata": {
    "height": 113,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate_ddpm = sample_ddpm(32)\n",
    "animation_ddpm = plot_sample(intermediate_ddpm,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ff242-8488-4cac-8fb0-68cbe3d4197a",
   "metadata": {},
   "source": [
    "#### View Epoch 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91dab98-3434-4e87-9b22-c062f11a724a",
   "metadata": {
    "height": 96,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_4.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4a136",
   "metadata": {
    "height": 113,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate_ddpm = sample_ddpm(32)\n",
    "animation_ddpm = plot_sample(intermediate_ddpm,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59d861e-6a09-4c93-92d2-07b2f66b6cbc",
   "metadata": {},
   "source": [
    "#### View Epoch 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a88f6d-f2c8-435a-ab86-349a60bc60e1",
   "metadata": {
    "height": 96,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_8.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d9172-ba9f-450f-be29-4751e4b5030e",
   "metadata": {
    "height": 113,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate_ddpm = sample_ddpm(32)\n",
    "animation_ddpm = plot_sample(intermediate_ddpm,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ae670d-2fe8-4dd5-80b9-648ebde01ac3",
   "metadata": {},
   "source": [
    "#### View Epoch 31 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1c223b-d830-4592-95fa-dea47d48685f",
   "metadata": {
    "height": 96,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_31.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1a1c4-3fae-4243-8682-80123773681b",
   "metadata": {
    "height": 113,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate_ddpm = sample_ddpm(32)\n",
    "animation_ddpm = plot_sample(intermediate_ddpm,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32cc4f4-d4e7-43d4-90d0-a89e4fbcc28c",
   "metadata": {},
   "source": [
    "# Acknowledgments\n",
    "Sprites by ElvGames, [FrootsnVeggies](https://zrghr.itch.io/froots-and-veggies-culinary-pixels) and  [kyrise](https://kyrise.itch.io/)   \n",
    "This code is modified from, https://github.com/cloneofsimo/minDiffusion   \n",
    "Diffusion model is based on [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) and [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b7532-3a24-4d8f-b175-284fd59dc037",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
