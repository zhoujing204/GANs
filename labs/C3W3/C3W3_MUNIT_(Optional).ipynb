{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C3W3: MUNIT (Optional).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1czVdIlqnImH"
      },
      "source": [
        "# Multi-modal Unsupervised Image-to-Image Translation (MUNIT)\n",
        "\n",
        "*Please note that this is an optional notebook, meant to introduce more advanced concepts if you're up for a challenge, so don't worry if you don't completely follow!*\n",
        "\n",
        "It is recommended that you should already be familiar with:\n",
        " - Layer Normalization, from [Layer Normalization](https://arxiv.org/abs/1607.06450) (Ba et al. 2016)\n",
        "\n",
        "### Goals\n",
        "\n",
        "In this notebook, you will learn about and implement MUNIT, a method for unsupervised image-to-image translation, as proposed in [Multimodal Unsupervised Image-to-Image Translation](https://arxiv.org/abs/1804.04732) (Huang et al. 2018).\n",
        "\n",
        "### Background\n",
        "\n",
        "MUNIT builds off UNIT's proposition of a shared latent space, but MUNIT only uses a partially shared latent space. Specifically, the authors assume that the content latent space is shared between two domains, but the style latent spaces are unique to each domain.\n",
        "\n",
        "Don't worry if you aren't familiar with UNIT - there will be a section that briefly goes over it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-UbOMuuD9Mt"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Let's begin with a quick overview of the UNIT framework and then move onto the MUNIT framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3PRXyfFNnBU"
      },
      "source": [
        "### UNIT\n",
        "\n",
        "UNIT, proposed in [Unsupervised Image-to-Image Translation Networks](https://arxiv.org/abs/1703.00848) (Liu et al. 2018), is a method of image translation that assumes that images from different domains share a latent distribution.\n",
        "\n",
        "Suppose that there are two image domains, $\\mathcal{A}$ and $\\mathcal{B}$. Images $(x_a, x_b) \\in (\\mathcal{A}, \\mathcal{B})$ can be mapped to a shared latent space, $\\mathcal{Z}$ via encoders $E_a: x_a \\mapsto z$ and $E_b: x_b \\mapsto z$, respectively. Synthetic images can be produced via generators $G_a: z \\mapsto x_a'$ and $G_b: z \\mapsto x_b'$, respectively. Note that the generators can generate self-reconstructed or domain-translated images for their respective domains.\n",
        "\n",
        "And as per all other GAN frameworks, synthetic and real images, $(x_a',x_a)$, and $(x_b', x_b)$, are passed into discriminators, $D_a$ and $D_b$, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpCebBIMNpMi"
      },
      "source": [
        "### MUNIT\n",
        "\n",
        "Suppose that there are two image domains, $\\mathcal{A}$ and $\\mathcal{B}$. A pair of corresponding images $(x_a, x_b) \\in (\\mathcal{A}, \\mathcal{B})$ can be generated as $x_a = F_a(c, s_a)$ and $x_b = F_b(c, s_b)$ where $c$ is a content vector from a shared distribution, $s_a, s_b$ are style vectors from distinct distributions, and $F_a, F_b$ are decoders that synthesize images from the content and style vectors.\n",
        "\n",
        "The idea is that while the content between two domains can be shared (i.e. you can interchange horses and zebras in an image), the styles are different between the two (i.e. you would draw horses and zebras differently).\n",
        "\n",
        "To learn the content and style distributions in training, the authors also assume some $E_a, E_b$ invert $F_a, F_b$, respectively. Specifically, $E_a^c: x_a \\mapsto c$ extracts content and $E_a^s: x_a \\mapsto s_a$ extracts style from images in domain $\\mathcal{A}$. The same applies for $E_b^c(x_b)$ and $E_b^s(x_b)$ with images in domain $\\mathcal{B}$. You can mix and match the content and style vectors from the two domains to translate images from between the two.\n",
        "\n",
        "For example, if you take content $b$, $c_b = E_b^c(x_b)$, and style $a$, $s_a = E_a^s(x_a)$, and pass these through the horse decoder as $F_a(c_b, s_a)$, you should end up with the image $b$ drawn with characteristics of image $a$.\n",
        "\n",
        "Don't worry if this is still unclear now! You'll go over this in more detail later in the notebook.\n",
        "\n",
        "![Same- and cross-domain interaction of encoders and decoders](https://github.com/https-deeplearning-ai/GANs-Public/blob/master/MUNIT-Domains.png?raw=true)\n",
        "\n",
        "*Model overview, taken from Figure 2 of [Multimodal Unsupervised Image-to-Image Translation](https://arxiv.org/abs/1804.04732) (Huang et al. 2018). The red and blue arrows denote encoders-decoder pairs within the same domain. Left: same domain image reconstruction. Right: cross-domain latent (content and style) vector reconstruction.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU8DDM6l9rZb"
      },
      "source": [
        "## Getting Started\n",
        "You will start by importing libraries and defining a visualization function. This code is borrowed from the CycleGAN notebook so you should already be familiar with this!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfkorNJrnmNO"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "def show_tensor_images(x_real, x_fake):\n",
        "    ''' For visualizing images '''\n",
        "    image_tensor = torch.cat((x_fake[:1, ...], x_real[:1, ...]), dim=0)\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat, nrow=1)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "import glob\n",
        "import random\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "# Inspired by https://github.com/aitorzip/PyTorch-CycleGAN/blob/master/datasets.py\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, transform=None, mode='train'):\n",
        "        super().__init__()\n",
        "        self.transform = transform\n",
        "        self.files_A = sorted(glob.glob(os.path.join(root, '%sA' % mode) + '/*.*'))\n",
        "        self.files_B = sorted(glob.glob(os.path.join(root, '%sB' % mode) + '/*.*'))\n",
        "        if len(self.files_A) > len(self.files_B):\n",
        "            self.files_A, self.files_B = self.files_B, self.files_A\n",
        "        self.new_perm()\n",
        "        assert len(self.files_A) > 0, \"Make sure you downloaded the horse2zebra images!\"\n",
        "\n",
        "    def new_perm(self):\n",
        "        self.randperm = torch.randperm(len(self.files_B))[:len(self.files_A)]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n",
        "        item_B = self.transform(Image.open(self.files_B[self.randperm[index]]))\n",
        "        if item_A.shape[0] != 3:\n",
        "            item_A = item_A.repeat(3, 1, 1)\n",
        "        if item_B.shape[0] != 3:\n",
        "            item_B = item_B.repeat(3, 1, 1)\n",
        "        if index == len(self) - 1:\n",
        "            self.new_perm()\n",
        "        return item_A, item_B\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.files_A), len(self.files_B))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1m6fuMHo_Y9",
        "outputId": "589dd916-4a97-45ba-aff0-d4f91f657292",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if len(os.listdir(\".\")) < 3:\n",
        "    !wget https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip\n",
        "    !unzip horse2zebra.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-18 13:16:21--  https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip\n",
            "Resolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190\n",
            "Connecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-05-18 13:16:22 ERROR 404: Not Found.\n",
            "\n",
            "unzip:  cannot find or open horse2zebra.zip, horse2zebra.zip.zip or horse2zebra.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQOT4X49Qh1S"
      },
      "source": [
        "## Subcomponents: Layers and Blocks\n",
        "\n",
        "MUNIT has a few key subcomponents that are used throughout the model. It'll generally make your life easier if you implement the smaller parts first!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE37vMTcQv6G"
      },
      "source": [
        "### Adaptive Instance Normalization (AdaIN)\n",
        "\n",
        "You've already learned about this layer in StyleGAN and seen a very similar cousin - class-conditional batch normalization - in the BigGAN components notebook.\n",
        "\n",
        "The authors enhance the linear layers for scale and shift with a multi-layer perceptron (MLP), which is essentially just a series of linear layers to help learn more complex representations. See the figure in **Submodules** and the notes in **Submodules: Decoder** for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N9KNJfoQ_-6"
      },
      "source": [
        "class AdaptiveInstanceNorm2d(nn.Module):\n",
        "    '''\n",
        "    AdaptiveInstanceNorm2d Class\n",
        "    Values:\n",
        "        channels: the number of channels the image has, a scalar\n",
        "        s_dim: the dimension of the style tensor (s), a scalar\n",
        "        h_dim: the hidden dimension of the MLP, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, channels, s_dim=8, h_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.instance_norm = nn.InstanceNorm2d(channels, affine=False)\n",
        "        self.style_scale_transform = self.mlp(s_dim, h_dim, channels)\n",
        "        self.style_shift_transform = self.mlp(s_dim, h_dim, channels)\n",
        "\n",
        "    @staticmethod\n",
        "    def mlp(self, in_dim, h_dim, out_dim):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(in_dim, h_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h_dim, h_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h_dim, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, image, w):\n",
        "        '''\n",
        "        Function for completing a forward pass of AdaIN: Given an image and a style,\n",
        "        returns the normalized image that has been scaled and shifted by the style.\n",
        "        Parameters:\n",
        "          image: the feature map of shape (n_samples, channels, width, height)\n",
        "          w: the intermediate noise vector w to be made into the style (y)\n",
        "        '''\n",
        "        normalized_image = self.instance_norm(image)\n",
        "        style_scale = self.style_scale_transform(w)[:, :, None, None]\n",
        "        style_shift = self.style_shift_transform(w)[:, :, None, None]\n",
        "        transformed_image = style_scale * normalized_image + style_shift\n",
        "        return transformed_image"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPf-WwjWlNiN"
      },
      "source": [
        "### Layer Normalization\n",
        "\n",
        "MUNIT uses layer normalization in the upsampling layers of the decoder. Proposed in [Layer Normalization](https://arxiv.org/abs/1607.06450) (Ba et al. 2016), layer normalization operates similarly to all other normalization techniques like batch normalization, but instead of normalizing across minibatch examples per channel, it normalizes across channels per minibatch example.\n",
        "\n",
        "Layer normalization is actually much more prevalent in NLP and but quite rare in computer vision. However, batch normalization is not viable here due to training batch sizes of 1 and instance normalization is undesirable because it normalizes the statistics at each position to a standard Gaussian, which removes style features.\n",
        "\n",
        "Pytorch implements this as [nn.LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) but requires precomputed spatial size for initialization to accomodate for 1D, 2D, and 3D inputs. For convenience, let's implement a size-agnostic layer normalization module for 2D inputs (i.e. images)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPHZpt-nmIEL"
      },
      "source": [
        "class LayerNorm2d(nn.Module):\n",
        "    '''\n",
        "    LayerNorm2d Class\n",
        "    Values:\n",
        "        channels: number of channels in input, a scalar\n",
        "        affine: whether to apply affine denormalization, a bool\n",
        "    '''\n",
        "\n",
        "    def __init__(self, channels, eps=1e-5, affine=True):\n",
        "        super().__init__()\n",
        "        self.affine = affine\n",
        "        self.eps = eps\n",
        "\n",
        "        if self.affine:\n",
        "            self.gamma = nn.Parameter(torch.rand(channels))\n",
        "            self.beta = nn.Parameter(torch.zeros(channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.flatten(1).mean(1).reshape(-1, 1, 1, 1)\n",
        "        std = x.flatten(1).std(1).reshape(-1, 1, 1, 1)\n",
        "\n",
        "        x = (x - mean) / (std + self.eps)\n",
        "\n",
        "        if self.affine:\n",
        "            x = x * self.gamma.reshape(1, -1, 1, 1) + self.beta.reshape(1, -1, 1, 1)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMWjIgMsKcGK"
      },
      "source": [
        "### Residual Block\n",
        "\n",
        "By now, you should already be very familiar with residual blocks. Below is an implementation that supports both adaptive and non-adaptive instance normalization layers, since both are used throughout the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtwbXUTRK4a9"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    '''\n",
        "    ResidualBlock Class\n",
        "    Values:\n",
        "        channels: number of channels throughout residual block, a scalar\n",
        "        s_dim: the dimension of the style tensor (s), a scalar\n",
        "        h_dim: the hidden dimension of the MLP, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, channels, s_dim=None, h_dim=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.utils.spectral_norm(\n",
        "                nn.Conv2d(channels, channels, kernel_size=3)\n",
        "            ),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.utils.spectral_norm(\n",
        "                nn.Conv2d(channels, channels, kernel_size=3)\n",
        "            ),\n",
        "        )\n",
        "        self.use_style = s_dim is not None and h_dim is not None\n",
        "        if self.use_style:\n",
        "            self.norm1 = AdaptiveInstanceNorm2d(channels, s_dim, h_dim)\n",
        "            self.norm2 = AdaptiveInstanceNorm2d(channels, s_dim, h_dim)\n",
        "        else:\n",
        "            self.norm1 = nn.InstanceNorm2d(channels)\n",
        "            self.norm2 = nn.InstanceNorm2d(channels)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, s=None):\n",
        "        x_id = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x, s) if self.use_style else self.norm1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x, s) if self.use_style else self.norm2(x)\n",
        "        return x + x_id"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faEGyHlHDmP9"
      },
      "source": [
        "## Submodules: Encoders and Decoder\n",
        "\n",
        "Now that you're all set up and implemented some basic building blocks, let's take a look at the content encoder, style encoder, and decoder! These will be used in the generator.\n",
        "\n",
        "![Same- and cross-domain interaction of encoders and decoders](https://github.com/https-deeplearning-ai/GANs-Public/blob/master/MUNIT-Generator.png?raw=true)\n",
        "\n",
        "*Generator architecture, taken from Figure 3 of [Multimodal Unsupervised Image-to-Image Translation](https://arxiv.org/abs/1804.04732) (Huang et al. 2018). Content encoder: generates a downsampled representation of the original. Style encoder: generates a style code from the original. Decoder: synthesizes a fake image from content code, infused with style info.*\n",
        "\n",
        "*Note: the official implementation feeds style code through a multi-layer perceptron (MLP) and assigns these values to the scale and shift parameters in the instance normalization layers. To be compatible with our previous definitions of* `AdaIN`*, your implementation will simply apply the MLP within* `AdaptiveInstanceNorm2d`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZoAZwZgHJcM"
      },
      "source": [
        "### Content Encoder\n",
        "\n",
        "The content encoder is similar to many encoders you've already seen: it simply downsamples the input image and feeds it through residual blocks to obtain a condensed representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCd393-QH_i0"
      },
      "source": [
        "class ContentEncoder(nn.Module):\n",
        "    '''\n",
        "    ContentEncoder Class\n",
        "    Values:\n",
        "        base_channels: number of channels in first convolutional layer, a scalar\n",
        "        n_downsample: number of downsampling layers, a scalar\n",
        "        n_res_blocks: number of residual blocks, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, base_channels=64, n_downsample=2, n_res_blocks=4):\n",
        "        super().__init__()\n",
        "\n",
        "        channels = base_channels\n",
        "\n",
        "        # Input convolutional layer\n",
        "        layers = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.utils.spectral_norm(\n",
        "                nn.Conv2d(3, channels, kernel_size=7)\n",
        "            ),\n",
        "            nn.InstanceNorm2d(channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "\n",
        "        # Downsampling layers\n",
        "        for i in range(n_downsample):\n",
        "            layers += [\n",
        "                nn.ReflectionPad2d(1),\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(channels, 2 * channels, kernel_size=4, stride=2)\n",
        "                ),\n",
        "                nn.InstanceNorm2d(2 * channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            channels *= 2\n",
        "\n",
        "        # Residual blocks\n",
        "        layers += [\n",
        "            ResidualBlock(channels) for _ in range(n_res_blocks)\n",
        "        ]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self.out_channels = channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "    @property\n",
        "    def channels(self):\n",
        "        return self.out_channels"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDaItt-rN9JD"
      },
      "source": [
        "### Style Encoder\n",
        "\n",
        "The style encoder operates similarly to the content encoder but instead of residual blocks, it uses global pooling and fully-connected layers to distill the input image to its style vector. An important difference is that the style encoder doesn't use any normalization layers, since they will remove the feature statistics that encode style. This style code will be passed to the decoder, which use this along with the content code to synthesize a fake image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cff4eEHOcLk"
      },
      "source": [
        "class StyleEncoder(nn.Module):\n",
        "    '''\n",
        "    StyleEncoder Class\n",
        "    Values:\n",
        "        base_channels: number of channels in first convolutional layer, a scalar\n",
        "        n_downsample: number of downsampling layers, a scalar\n",
        "        s_dim: the dimension of the style tensor (s), a scalar\n",
        "    '''\n",
        "\n",
        "    n_deepen_layers = 2\n",
        "\n",
        "    def __init__(self, base_channels=64, n_downsample=4, s_dim=8):\n",
        "        super().__init__()\n",
        "\n",
        "        channels = base_channels\n",
        "\n",
        "        # Input convolutional layer\n",
        "        layers = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.utils.spectral_norm(\n",
        "                nn.Conv2d(3, channels, kernel_size=7, padding=0)\n",
        "            ),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "\n",
        "        # Downsampling layers\n",
        "        for i in range(self.n_deepen_layers):\n",
        "            layers += [\n",
        "                nn.ReflectionPad2d(1),\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(channels, 2 * channels, kernel_size=4, stride=2)\n",
        "                ),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            channels *= 2\n",
        "        for i in range(n_downsample - self.n_deepen_layers):\n",
        "            layers += [\n",
        "                nn.ReflectionPad2d(1),\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(channels, channels, kernel_size=4, stride=2)\n",
        "                ),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "\n",
        "        # Apply global pooling and pointwise convolution to style_channels\n",
        "        layers += [\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(channels, s_dim, kernel_size=1),\n",
        "        ]\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW9B-i2tP1fM"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "As with all encoder-decoder frameworks, the decoder serves to synthesize images from the latent information passed through by the encoder. In this case, the decoder works with both content and style encodings.\n",
        "\n",
        "You can think of the content encoder and decoder as the backbone of the encoder-decoder framework with style information injected into the residual blocks via `AdaIN` layers.\n",
        "\n",
        "Note: the official implementation feeds style code through a multi-layer perceptron (MLP) and assigns these values to the scale and shift parameters in the instance normalization layers. To be compatible with the previous definitions of `AdaIN` in this course, your implementation will simply apply the MLP within `AdaptiveInstanceNorm2d`.\n",
        "\n",
        "Let's take a look at the implementation!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf8KP7DRRrkn"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    '''\n",
        "    Decoder Class\n",
        "    Values:\n",
        "        in_channels: number of channels from encoder output, a scalar\n",
        "        n_upsample: number of upsampling layers, a scalar\n",
        "        n_res_blocks: number of residual blocks, a scalar\n",
        "        s_dim: the dimension of the style tensor (s), a scalar\n",
        "        h_dim: the hidden dimension of the MLP, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_channels, n_upsample=2, n_res_blocks=4, s_dim=8, h_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        channels = in_channels\n",
        "\n",
        "        # Residual blocks with AdaIN\n",
        "        self.res_blocks = nn.ModuleList([\n",
        "            ResidualBlock(channels, s_dim) for _ in range(n_res_blocks)\n",
        "        ])\n",
        "\n",
        "        # Upsampling blocks\n",
        "        layers = []\n",
        "        for i in range(n_upsample):\n",
        "            layers += [\n",
        "                nn.Upsample(scale_factor=2),\n",
        "                nn.ReflectionPad2d(2),\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(channels, channels // 2, kernel_size=5)\n",
        "                ),\n",
        "                LayerNorm2d(channels // 2),\n",
        "            ]\n",
        "            channels //= 2\n",
        "\n",
        "        layers += [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.utils.spectral_norm(\n",
        "                nn.Conv2d(channels, 3, kernel_size=7)\n",
        "            ),\n",
        "            nn.Tanh(),\n",
        "        ]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        for res_block in self.res_blocks:\n",
        "            x = res_block(x, s=s)\n",
        "        x = self.layers(x)\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ru1dJWZ0XGK"
      },
      "source": [
        "## Modules: Generator, Discriminator, and Loss\n",
        "\n",
        "Now you're ready to implement MUNIT generator and discriminator, as well as the composite loss function that ties everything together in training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8-de36ytk14"
      },
      "source": [
        "### Generator\n",
        "\n",
        "The generator is essentially just comprised of the two encoders and one decoder implemented in the previous section, so let's wrap everything in a `Generator` module!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl_ZpENltpL_"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    '''\n",
        "    Generator Class\n",
        "    Values:\n",
        "        base_channels: number of channels in first convolutional layer, a scalar\n",
        "        n_downsample: number of downsampling layers, a scalar\n",
        "        n_res_blocks: number of residual blocks, a scalar\n",
        "        s_dim: the dimension of the style tensor (s), a scalar\n",
        "        h_dim: the hidden dimension of the MLP, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_channels: int = 64,\n",
        "        n_c_downsample: int = 2,\n",
        "        n_s_downsample: int = 4,\n",
        "        n_res_blocks: int = 4,\n",
        "        s_dim: int = 8,\n",
        "        h_dim: int = 256,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.c_enc = ContentEncoder(\n",
        "            base_channels=base_channels, n_downsample=n_c_downsample, n_res_blocks=n_res_blocks,\n",
        "        )\n",
        "        self.s_enc = StyleEncoder(\n",
        "            base_channels=base_channels, n_downsample=n_s_downsample, s_dim=s_dim,\n",
        "        )\n",
        "        self.dec = Decoder(\n",
        "            self.c_enc.channels, n_upsample=n_c_downsample, n_res_blocks=n_res_blocks, s_dim=s_dim, h_dim=h_dim,\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        content = self.c_enc(x)\n",
        "        style = self.s_enc(x)\n",
        "        return (content, style)\n",
        "\n",
        "    def decode(self, content, style):\n",
        "        return self.dec(content, style)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE3H9XlVxuDM"
      },
      "source": [
        "### Discriminator\n",
        "\n",
        "The discriminator, identical to the one used in Pix2PixHD, is comprised of several PatchGAN discriminators operating at different scales. For details on how these work together, take a look at the Pix2PixHD optional notebook.\n",
        "\n",
        "The discriminator is trained with the least squares objective."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI-JT2VsyJil"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    '''\n",
        "    Generator Class\n",
        "    Values:\n",
        "        base_channels: number of channels in first convolutional layer, a scalar\n",
        "        n_layers: number of downsampling layers, a scalar\n",
        "        n_discriminators: number of discriminators (all at different scales), a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_channels: int = 64,\n",
        "        n_layers: int = 3,\n",
        "        n_discriminators: int = 3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            self.patchgan_discriminator(base_channels, n_layers) for _ in range(n_discriminators)\n",
        "        ])\n",
        "\n",
        "        self.downsample = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def patchgan_discriminator(base_channels, n_layers):\n",
        "        '''\n",
        "        Function that constructs and returns one PatchGAN discriminator module.\n",
        "        '''\n",
        "        channels = base_channels\n",
        "        # Input convolutional layer\n",
        "        layers = [\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.utils.spectral_norm(\n",
        "                nn.Conv2d(3, channels, kernel_size=4, stride=2),\n",
        "            ),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        ]\n",
        "\n",
        "        # Hidden convolutional layers\n",
        "        for _ in range(n_layers):\n",
        "            layers += [\n",
        "                nn.ReflectionPad2d(1),\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(channels, 2 * channels, kernel_size=4, stride=2)\n",
        "                ),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            ]\n",
        "            channels *= 2\n",
        "\n",
        "        # Output projection layer\n",
        "        layers += [\n",
        "            nn.utils.spectral_norm(\n",
        "                nn.Conv2d(channels, 1, kernel_size=1)\n",
        "            ),\n",
        "        ]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for discriminator in self.discriminators:\n",
        "            outputs.append(discriminator(x))\n",
        "            x = self.downsample(x)\n",
        "        return outputs"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdafpxKB089k"
      },
      "source": [
        "### Loss Functions\n",
        "\n",
        "There are a lot of moving parts in MUNIT so this section will break down which parts interact with which other parts. Recall from earlier our notation:\n",
        " - Image domains:\n",
        "    \\begin{align*}\n",
        "        a &\\in \\mathcal{A} \\\\\n",
        "        b &\\in \\mathcal{B}\n",
        "    \\end{align*}\n",
        " - Encoders ($E$):\n",
        "    \\begin{align*}\n",
        "        E_a^c: a \\mapsto c_a, &\\quad E_a^s: a \\mapsto s_a \\\\\n",
        "        E_b^c: b \\mapsto c_b, &\\quad E_b^s: b \\mapsto s_b\n",
        "    \\end{align*}\n",
        " - Decoders ($F$):\n",
        "    \\begin{align*}\n",
        "        F_a&: (c_*, s_a) \\mapsto a' \\\\\n",
        "        F_b&: (c_*, s_b) \\mapsto b'\n",
        "    \\end{align*}\n",
        " - Generators ($G$):\n",
        "    \\begin{align*}\n",
        "        G_a(a, b) &= F_a(E_b^c(b), E_a^s(a)) \\\\\n",
        "        G_b(b, a) &= F_b(E_a^c(a), E_b^s(b))\n",
        "    \\end{align*}\n",
        " - Discriminators ($D$):\n",
        "    \\begin{align*}\n",
        "        D_a&: a' \\mapsto p \\in \\mathbb{R} \\\\\n",
        "        D_b&: b' \\mapsto p \\in \\mathbb{R}\n",
        "    \\end{align*}\n",
        "\n",
        "**Image Reconstruction Loss**\n",
        "\n",
        "The model should be able to encode and decode a reconstruction of the image. For domain $\\mathcal{A}$, this can be expressed as\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathcal{L}_{\\text{recon}}^a &= \\mathbb{E}_{a\\sim p(a)}\\left|\\left|F_a(E_a^c(a), E_a^s(a)) - a\\right|\\right|_1\n",
        "\\end{align*}\n",
        "\n",
        "and for domain $\\mathcal{B}$, this can be expressed as\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathcal{L}_{\\text{recon}}^b &= \\mathbb{E}_{b\\sim p(b)}\\left|\\left|F_b(E_b^c(b), E_b^s(b)) - b\\right|\\right|_1.\n",
        "\\end{align*}\n",
        "\n",
        "**Latent Reconstruction Loss**\n",
        "\n",
        "The same principle from above applies to the latent space: decoding and encoding a latent vector should reproduce the original. Don't worry if the equations look complicated! Just know that intuitively, passing in different content and style vectors through the encoders should yield those same input vectors. For domain $\\mathcal{A}$, this can be expressed as\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathcal{L}_{\\text{recon}}^{c_b} &= \\mathbb{E}_{c_b\\sim p(c_b),s_a\\sim q(s_a)}\\left|\\left|E_a^c(F_a(c_b, s_a)) - c_a\\right|\\right|_1 \\\\\n",
        "    \\mathcal{L}_{\\text{recon}}^{s_a} &= \\mathbb{E}_{c_b\\sim p(c_b),s_a\\sim q(s_a)}\\left|\\left|E_a^s(F_a(c_b, s_a)) - s_b\\right|\\right|_1\n",
        "\\end{align*}\n",
        "\n",
        "and for domain $\\mathcal{B}$, this can be expressed as\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathcal{L}_{\\text{recon}}^{c_a} &= \\mathbb{E}_{c_a\\sim p(c_a),s_b\\sim q(s_b)}\\left|\\left|E_b^c(F_b(c_a, s_b)) - c_b\\right|\\right|_1 \\\\\n",
        "    \\mathcal{L}_{\\text{recon}}^{s_b} &= \\mathbb{E}_{c_a\\sim p(c_a),s_b\\sim q(s_b)}\\left|\\left|E_b^s(F_b(c_a, s_b)) - s_a\\right|\\right|_1\n",
        "\\end{align*}\n",
        "\n",
        "**Adversarial Loss**\n",
        "\n",
        "As with all other GANs, MUNIT is trained with adversarial loss. The authors opt for the LSGAN least-squares objective. For domain $\\mathcal{A}$, this can be expressed as\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathcal{L}_{\\text{GAN}}^a &= \\mathbb{E}_{c_b\\sim p(c_b),s_a\\sim q(s_a)}\\left[(1 - D_a(G_a(c_b, s_a)))^2\\right] + \\mathbb{E}_{a\\sim p(a)}\\left[D_a(a)^2\\right]\n",
        "\\end{align*}\n",
        "\n",
        "and for domain $\\mathcal{B}$, this can be expressed as\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathcal{L}_{\\text{GAN}}^b &= \\mathbb{E}_{c_a\\sim p(c_a),s_b\\sim q(s_b)}\\left[(1 - D_b(G_b(c_a, s_b)))^2\\right] + \\mathbb{E}_{b\\sim p(b)}\\left[D_b(b)^2\\right]\n",
        "\\end{align*}\n",
        "\n",
        "**Total Loss**\n",
        "\n",
        "The total loss can now be expressed in terms of the individual losses from above, so the objective can be expressed as\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathcal{L}(E_a, E_b, F_a, F_b, D_a, D_b) &= \\mathcal{L}_{\\text{GAN}}^a + \\mathcal{L}_{\\text{GAN}}^b + \\lambda_x(\\mathcal{L}_{\\text{recon}}^a + \\mathcal{L}_{\\text{recon}}^b) + \\lambda_c(\\mathcal{L}_{\\text{recon}}^{c_a} + \\mathcal{L}_{\\text{recon}}^{c_b}) + \\lambda_s(\\mathcal{L}_{\\text{recon}}^{s_a} + \\mathcal{L}_{\\text{recon}}^{s_b})\n",
        "\\end{align*}\n",
        "\n",
        "And now the fun part: implementing this ginormous composite loss!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSZDUYjNWXgi"
      },
      "source": [
        "class GinormousCompositeLoss(nn.Module):\n",
        "    '''\n",
        "    GinormousCompositeLoss Class: implements all losses for MUNIT\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def image_recon_loss(x, gen):\n",
        "        c, s = gen.encode(x)\n",
        "        recon = gen.decode(c, s)\n",
        "        return F.l1_loss(recon, x), c, s\n",
        "\n",
        "    @staticmethod\n",
        "    def latent_recon_loss(c, s, gen):\n",
        "        x_fake = gen.decode(c, s)\n",
        "        recon = gen.encode(x_fake)\n",
        "        return F.l1_loss(recon[0], c), F.l1_loss(recon[1], s), x_fake\n",
        "\n",
        "    @staticmethod\n",
        "    def adversarial_loss(x, dis, is_real):\n",
        "        preds = dis(x)\n",
        "        target = torch.ones_like if is_real else torch.zeros_like\n",
        "        loss = 0.0\n",
        "        for pred in preds:\n",
        "            loss += F.mse_loss(pred, target(pred))\n",
        "        return loss"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gttCYcvt-cTX"
      },
      "source": [
        "### Optional Loss Functions\n",
        "\n",
        "The authors also propose a couple of auxiliary loss functions that can help improve convergence. This section will just go over these losses but implementation is left as an exercise for the curious reader :)\n",
        "\n",
        "**Style-augmented Cycle Consistency**\n",
        "\n",
        "You've already heard of cycle consistency from CycleGAN, which implies that an image translated to the target domain and back should be identical to the original.\n",
        "\n",
        "Intuitively, style-augmented cycle consistency implies that an image translated to the target domain and back using the original style should result in the original image. Style-augmented cycle consistency is implicitly encouraged\n",
        "by the reconstruction losses, but the authors note that explicitly enforcing it could be useful in some cases.\n",
        "\n",
        "**Domain Invariant Perceptual Loss**\n",
        "\n",
        "You're probably already familiar with perceptual loss, which is usually implemented via MSE loss between feature maps of fake and real images. However, because the images in the domains are unpaired, pixel-wise loss may not be optimal, since each values at each position do not correspond spatially.\n",
        "\n",
        "The authors get around this discrepancy by applying instance normalization to the feature maps. This normalizes the values per channel for each position in the feature maps, so MSE loss penalizes the difference in statistics rather than raw pixel value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jbwOHEgffdx"
      },
      "source": [
        "## Model: MUNIT\n",
        "\n",
        "Now that you've got all the necessary modules, let's see how we can put them all together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idFCK4ugfoUo"
      },
      "source": [
        "class MUNIT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        gen_channels: int = 64,\n",
        "        n_c_downsample: int = 2,\n",
        "        n_s_downsample: int = 4,\n",
        "        n_res_blocks: int = 4,\n",
        "        s_dim: int = 8,\n",
        "        h_dim: int = 256,\n",
        "        dis_channels: int = 64,\n",
        "        n_layers: int = 3,\n",
        "        n_discriminators: int = 3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gen_a = Generator(\n",
        "            base_channels=gen_channels, n_c_downsample=n_c_downsample, n_s_downsample=n_s_downsample, n_res_blocks=n_res_blocks, s_dim=s_dim, h_dim=h_dim,\n",
        "        )\n",
        "        self.gen_b = Generator(\n",
        "            base_channels=gen_channels, n_c_downsample=n_c_downsample, n_s_downsample=n_s_downsample, n_res_blocks=n_res_blocks, s_dim=s_dim, h_dim=h_dim,\n",
        "        )\n",
        "        self.dis_a = Discriminator(\n",
        "            base_channels=dis_channels, n_layers=n_layers, n_discriminators=n_discriminators,\n",
        "        )\n",
        "        self.dis_b = Discriminator(\n",
        "            base_channels=dis_channels, n_layers=n_layers, n_discriminators=n_discriminators,\n",
        "        )\n",
        "        self.s_dim = s_dim\n",
        "        self.loss = GinormousCompositeLoss\n",
        "\n",
        "    def forward(self, x_a, x_b):\n",
        "        s_a = torch.randn(x_a.size(0), self.s_dim, 1, 1, device=x_a.device).to(x_a.dtype)\n",
        "        s_b = torch.randn(x_b.size(0), self.s_dim, 1, 1, device=x_b.device).to(x_b.dtype)\n",
        "\n",
        "        # Encode real x and compute image reconstruction loss\n",
        "        x_a_loss, c_a, s_a_fake = self.loss.image_recon_loss(x_a, self.gen_a)\n",
        "        x_b_loss, c_b, s_b_fake = self.loss.image_recon_loss(x_b, self.gen_b)\n",
        "\n",
        "        # Decode real (c, s) and compute latent reconstruction loss\n",
        "        c_b_loss, s_a_loss, x_ba = self.loss.latent_recon_loss(c_b, s_a, self.gen_a)\n",
        "        c_a_loss, s_b_loss, x_ab = self.loss.latent_recon_loss(c_a, s_b, self.gen_b)\n",
        "\n",
        "        # Compute adversarial losses\n",
        "        gen_a_adv_loss = self.loss.adversarial_loss(x_ba, self.dis_a, True)\n",
        "        gen_b_adv_loss = self.loss.adversarial_loss(x_ab, self.dis_b, True)\n",
        "\n",
        "        # Sum up losses for gen\n",
        "        gen_loss = (\n",
        "            10 * x_a_loss + c_b_loss + s_a_loss + gen_a_adv_loss + \\\n",
        "            10 * x_b_loss + c_a_loss + s_b_loss + gen_b_adv_loss\n",
        "        )\n",
        "\n",
        "        # Sum up losses for dis\n",
        "        dis_loss = (\n",
        "            self.loss.adversarial_loss(x_ba.detach(), self.dis_a, False) + \\\n",
        "            self.loss.adversarial_loss(x_a.detach(), self.dis_a, True) + \\\n",
        "            self.loss.adversarial_loss(x_ab.detach(), self.dis_b, False) + \\\n",
        "            self.loss.adversarial_loss(x_b.detach(), self.dis_b, True)\n",
        "        )\n",
        "\n",
        "        return gen_loss, dis_loss, x_ab, x_ba"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHz9F4m9wvKO"
      },
      "source": [
        "### Training\n",
        "\n",
        "Now you're ready to train MUNIT! Let's start by setting some optimization parameters and initializing everything you'll need for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYQrHbgPw5ie",
        "outputId": "69cb0d86-f52b-4aa4-90ec-f65467aa5e53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "# Initialize model\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
        "\n",
        "munit_config = {\n",
        "    'gen_channels': 64,\n",
        "    'n_c_downsample': 2,\n",
        "    'n_s_downsample': 4,\n",
        "    'n_res_blocks': 4,\n",
        "    's_dim': 8,\n",
        "    'h_dim': 256,\n",
        "    'dis_channels': 64,\n",
        "    'n_layers': 3,\n",
        "    'n_discriminators': 3,\n",
        "}\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "munit = MUNIT(**munit_config).to(device).apply(weights_init)\n",
        "\n",
        "# Initialize dataloader\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(286),\n",
        "    transforms.RandomCrop(256),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5), (0.5))\n",
        "])\n",
        "dataloader = DataLoader(\n",
        "    ImageDataset('horse2zebra', transform),\n",
        "    batch_size=1, pin_memory=True, shuffle=True,\n",
        ")\n",
        "\n",
        "# Initialize optimizers\n",
        "gen_params = list(munit.gen_a.parameters()) + list(munit.gen_b.parameters())\n",
        "dis_params = list(munit.dis_a.parameters()) + list(munit.dis_b.parameters())\n",
        "gen_optimizer = torch.optim.Adam(gen_params, lr=1e-4, betas=(0.5, 0.999))\n",
        "dis_optimizer = torch.optim.Adam(dis_params, lr=1e-4, betas=(0.5, 0.999))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Make sure you downloaded the horse2zebra images!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-99e8c83fbfa1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m ])\n\u001b[1;32m     28\u001b[0m dataloader = DataLoader(\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mImageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'horse2zebra'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-1-8a75923fac5f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, mode)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles_A\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_perm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles_A\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Make sure you downloaded the horse2zebra images!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_perm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Make sure you downloaded the horse2zebra images!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9d8AneYxuuT"
      },
      "source": [
        "# Parse torch version for autocast\n",
        "# ######################################################\n",
        "version = torch.__version__\n",
        "version = tuple(int(n) for n in version.split('.')[:-1])\n",
        "has_autocast = version >= (1, 6)\n",
        "# ######################################################\n",
        "\n",
        "def train(munit, dataloader, optimizers, device):\n",
        "\n",
        "    max_iters = 1000000\n",
        "    decay_every = 100000\n",
        "    cur_iter = 0\n",
        "\n",
        "    display_every = 500\n",
        "    mean_losses = [0., 0.]\n",
        "\n",
        "    while cur_iter < max_iters:\n",
        "        for (x_a, x_b) in tqdm(dataloader):\n",
        "            x_a = x_a.to(device)\n",
        "            x_b = x_b.to(device)\n",
        "\n",
        "            # Enable autocast to FP16 tensors (new feature since torch==1.6.0)\n",
        "            # If you're running older versions of torch, comment this out\n",
        "            # and use NVIDIA apex for mixed/half precision training\n",
        "            if has_autocast:\n",
        "                with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
        "                    outputs = munit(x_a, x_b)\n",
        "            else:\n",
        "                outputs = munit(x_a, x_b)\n",
        "\n",
        "            losses, x_ab, x_ba = outputs[:-2], outputs[-2], outputs[-1]\n",
        "            munit.zero_grad()\n",
        "\n",
        "            for i, (optimizer, loss) in enumerate(zip(optimizers, losses)):\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                mean_losses[i] += loss.item() / display_every\n",
        "\n",
        "            cur_iter += 1\n",
        "\n",
        "            if cur_iter % display_every == 0:\n",
        "                print('Step {}: [G loss: {:.5f}][D loss: {:.5f}]'\n",
        "                      .format(cur_iter, *mean_losses))\n",
        "                show_tensor_images(x_ab, x_a)\n",
        "                show_tensor_images(x_ba, x_b)\n",
        "                mean_losses = [0., 0.]\n",
        "\n",
        "            if cur_iter == max_iters:\n",
        "                break\n",
        "\n",
        "            # Schedule learning rate by 0.5\n",
        "            if cur_iter % decay_every == 0:\n",
        "                for optimizer in optimizers:\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        param_group['lr'] *= 0.5\n",
        "\n",
        "train(\n",
        "    munit, dataloader,\n",
        "    [gen_optimizer, dis_optimizer],\n",
        "    device,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}